{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification using Char-level Embedding on Large Movie Reviews\n",
    "\n",
    "[Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is understood as a classic natural language processing problem. In this example, a large moview review dataset was chosen from IMDB to do a sentiment classification task with some deep learning approaches. The labeled data set consists of 50,000 [IMDB](http://www.imdb.com/) movie reviews (good or bad), in which 25000 highly polar movie reviews for training, and 25,000 for testing. The dataset is originally collected by Stanford researchers and was used in a [2011 paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf), and the highest accuray of 88.33% was achieved without using the unbalanced data. This example illustrates some deep learning approaches to do the sentiment classification with [BigDL](https://github.com/intel-analytics/BigDL) python API.\n",
    "\n",
    "### Load the IMDB Dataset\n",
    "The IMDB dataset need to be loaded into BigDL, note that the dataset has been pre-processed, and each review was encoded as a sequence of integers. Each integer represents the index of the overall frequency of dataset, for instance, '5' means the 5-th most frequent words occured in the data. It is very convinient to filter the words by some conditions, for example, to filter only the top 5,000 most common word and/or eliminate the top 30 most common words. Let's define functions to load the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "finished processing text\n",
      "Processing vocabulary\n",
      "finished processing vocabulary\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dataset import base\n",
    "import numpy as np\n",
    "\n",
    "def download_imdb(dest_dir):\n",
    "    \"\"\"Download pre-processed IMDB movie review data\n",
    "\n",
    "    :argument\n",
    "        dest_dir: destination directory to store the data\n",
    "\n",
    "    :return\n",
    "        The absolute path of the stored data\n",
    "    \"\"\"\n",
    "    file_name = \"imdb.npz\"\n",
    "    file_abs_path = base.maybe_download(file_name,\n",
    "                                        dest_dir,\n",
    "                                        'https://s3.amazonaws.com/text-datasets/imdb.npz')\n",
    "    return file_abs_path\n",
    "\n",
    "def load_imdb(dest_dir='~/'):\n",
    "    \"\"\"Load IMDB dataset.\n",
    "\n",
    "    :argument\n",
    "        dest_dir: where to cache the data (relative to `~/.bigdl/dataset`).\n",
    "\n",
    "    :return\n",
    "        the train, test separated IMDB dataset.\n",
    "    \"\"\"\n",
    "#     path = download_imdb(dest_dir)\n",
    "    path = \"imdb.npz\"\n",
    "    f = np.load(path)\n",
    "    x_train = f['x_train']\n",
    "    y_train = f['y_train']\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "    f.close()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "print('Processing text dataset')\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb()\n",
    "print('finished processing text')\n",
    "\n",
    "import json\n",
    "\n",
    "def get_word_index(dest_dir='/tmp/.bigdl/dataset', ):\n",
    "    \"\"\"Retrieves the dictionary mapping word indices back to words.\n",
    "\n",
    "    :argument\n",
    "        path: where to cache the data (relative to `~/.bigdl/dataset`).\n",
    "\n",
    "    :return\n",
    "        The word index dictionary.\n",
    "    \"\"\"\n",
    "    file_name = \"imdb_word_index.json\"\n",
    "    path = base.maybe_download(file_name,\n",
    "                               dest_dir,\n",
    "                               source_url='https://s3.amazonaws.com/text-datasets/imdb_word_index.json')\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "print('Processing vocabulary')\n",
    "word_idx = get_word_index()\n",
    "idx_word = {v:k for k,v in word_idx.items()}\n",
    "print('finished processing vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "\n",
    "Before we train the network, some pre-processing steps need to be applied to the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov(x, oov_char, max_words):\n",
    "    \"\"\"\n",
    "    Replace the words out of vocabulary with `oov_char`\n",
    "    :param x: a sequence\n",
    "    :param max_words: the max number of words to include\n",
    "    :param oov_char: words out of vocabulary because of exceeding the `max_words`\n",
    "        limit will be replaced by this character\n",
    "\n",
    "    :return: The replaced sequence\n",
    "    \"\"\"\n",
    "    return [oov_char if w >= max_words else w for w in x]\n",
    "\n",
    "def pad_sequence(x, fill_value, length):\n",
    "    \"\"\"\n",
    "    Pads each sequence to the same length\n",
    "    :param x: a sequence\n",
    "    :param fill_value: pad the sequence with this value\n",
    "    :param length: pad sequence to the length\n",
    "\n",
    "    :return: the padded sequence\n",
    "    \"\"\"\n",
    "    if len(x) >= length:\n",
    "        return x[(len(x) - length):]\n",
    "    else:\n",
    "        return [fill_value] * (length - len(x)) + x\n",
    "\n",
    "def transform_char(w, idx_word, char_padding_value, word_len, UNK_char_value):\n",
    "    \"\"\"\n",
    "    Transfer each word to char sequence\n",
    "    :param x: a sequence\n",
    "    :param fill_value: pad the sequence with this value\n",
    "    :param length: pad sequence to the length\n",
    "\n",
    "    :return: the padded sequence\n",
    "    \"\"\"\n",
    "    word = idx_word.get(w - index_from, 0)\n",
    "    if not word:\n",
    "        l = [char_idx.get(c, UNK_char_value) for c in word]\n",
    "        return pad_sequence(l, char_padding_value, word_len)\n",
    "    else:\n",
    "        return [char_padding_value] * word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start transformation\n",
      "finish transformation\n"
     ]
    }
   ],
   "source": [
    "print('start transformation')\n",
    "\n",
    "from zoo.common.nncontext import *\n",
    "sc = init_nncontext(\"Sentiment Analysis Example\")\n",
    "\n",
    "padding_value = 1\n",
    "start_char = 2\n",
    "oov_char = 3\n",
    "index_from = 3\n",
    "max_words = 5000\n",
    "sequence_len = 500\n",
    "char_input_length = 16\n",
    "char_dim = 100\n",
    "char_input_dim = 30\n",
    "char_output_dim = 100\n",
    "char_padding_value = 0\n",
    "UNK_char_value = 27\n",
    "\n",
    "char_idx = {}\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "for idx, char in enumerate(alphabet):\n",
    "    char_idx[char] = idx + 1\n",
    "    \n",
    "merged_train_rdd = sc.parallelize(zip(x_train, y_train), 2) \\\n",
    "    .map(lambda record: ([start_char] + [w + index_from for w in record[0]], record[1])) \\\n",
    "    .map(lambda record: (replace_oov(record[0], oov_char, max_words), record[1])) \\\n",
    "    .map(lambda record: (pad_sequence(record[0], padding_value, sequence_len), record[1])) \\\n",
    "    .map(lambda record: Sample.from_ndarray([np.array(record[0]), np.array([transform_char(w, idx_word, 0, char_input_length, 27) for w in record[0]])], np.array(record[1])))\n",
    "\n",
    "merged_test_rdd = sc.parallelize(zip(x_test, y_test), 2) \\\n",
    "    .map(lambda record: ([start_char] + [w + index_from for w in record[0]], record[1])) \\\n",
    "    .map(lambda record: (replace_oov(record[0], oov_char, max_words), record[1])) \\\n",
    "    .map(lambda record: (pad_sequence(record[0], padding_value, sequence_len), record[1])) \\\n",
    "    .map(lambda record: Sample.from_ndarray([np.array(record[0]), np.array([transform_char(w, idx_word, 0, char_input_length, 27) for w in record[0]])], np.array(record[1])))\n",
    "\n",
    "print('finish transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "[Word embedding](https://en.wikipedia.org/wiki/Word_embedding) is a recent breakthrough in natural language field. The key idea is to encode words and phrases into distributed representations in the format of word vectors, which means each word is represented as a vector. There are two widely used word vector training alogirhms, one is published by Google called [word to vector](https://arxiv.org/abs/1310.4546), the other is published by Standford called [Glove](https://nlp.stanford.edu/projects/glove/). In this example, pre-trained glove is loaded into a lookup table and will be fine-tuned during the training process. BigDL provides a method to download and load glove in `news20` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading glove\n",
      "finish loading glove\n",
      "processing glove\n",
      "finish processing glove\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dataset import news20\n",
    "import itertools\n",
    "\n",
    "word_embedding_dim = 100\n",
    "\n",
    "print('loading glove')\n",
    "glove = news20.get_glove_w2v(source_dir='/tmp/.bigdl/dataset', dim=word_embedding_dim)\n",
    "print('finish loading glove')\n",
    "\n",
    "print('processing glove')\n",
    "w2v = [glove.get(idx_word.get(i - index_from), np.random.uniform(-0.05, 0.05, word_embedding_dim))\n",
    "        for i in range(1, max_words + 1)]\n",
    "w2v = np.array(list(itertools.chain(*np.array(w2v, dtype='float'))), dtype='float') \\\n",
    "        .reshape([max_words, word_embedding_dim])\n",
    "print('finish processing glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models\n",
    "\n",
    "Next, let's build the GRU model with word and char-level embedding for the sentiment classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasInput\n",
      "creating: createZooKerasInput\n",
      "creating: createZooKerasCharEmbedding\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasEmbedding\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasTimeDistributed\n",
      "creating: createZooKerasMerge\n",
      "creating: createZooKerasELU\n",
      "creating: createZooKerasGRU\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasDropout\n",
      "creating: createZooKerasDense\n"
     ]
    }
   ],
   "source": [
    "from zoo.pipeline.api.keras.layers import *\n",
    "from zoo.pipeline.api.keras.models import *\n",
    "\n",
    "word_input = Input(shape=(500,))\n",
    "char_input = Input(shape=(500, char_input_length))\n",
    "\n",
    "char_emb = TimeDistributed(CharEmbedding(input_dim=char_input_dim, output_dim=char_output_dim, \\\n",
    "                                         char_embed_dim=char_dim, input_length=char_input_length))(char_input)\n",
    "# word_emb = Embedding(embedding_file=\"/tmp/.bigdl/dataset/glove.6B.100d.txt\", word_index=word_idx , trainable=False)(word_input)\n",
    "word_emb = Embedding(input_dim=max_words, output_dim=word_embedding_dim, input_length=sequence_len, )\n",
    "emb = word_emb(word_input)\n",
    "word_emb.set_weights(np.array([w2v]))\n",
    "word_emb.trainable = False\n",
    "\n",
    "x1 = TimeDistributed(Dense(100))(char_emb)\n",
    "x2 = TimeDistributed(Dense(100))(emb)\n",
    "x = merge(inputs=[x1, x2], mode = \"sum\" )\n",
    "x = ELU()(x)\n",
    "# x = merge(inputs=[char_emb, emb], mode = \"concat\")\n",
    "\n",
    "x = GRU(128)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasModel\n"
     ]
    }
   ],
   "source": [
    "inputs = [word_input, char_input]\n",
    "model = Model(input=inputs, output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createBCECriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdam\n",
      "creating: createDistriOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n",
      "creating: createTrainSummary\n",
      "creating: createSeveralIteration\n",
      "creating: createValidationSummary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.optim.optimizer.Optimizer at 0x7f1cb0f755f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.nn.criterion import *\n",
    "\n",
    "max_epoch = 4\n",
    "batch_size = 56\n",
    "model_type = 'lstm'\n",
    "\n",
    "\n",
    "optimizer = Optimizer(\n",
    "        model=model,\n",
    "        training_rdd=merged_train_rdd,\n",
    "        criterion=BCECriterion(),\n",
    "        end_trigger=MaxEpoch(max_epoch),\n",
    "        batch_size=batch_size,\n",
    "        optim_method=Adam())\n",
    "\n",
    "optimizer.set_validation(\n",
    "        batch_size=batch_size,\n",
    "        val_rdd=merged_test_rdd,\n",
    "        trigger=EveryEpoch(),\n",
    "        val_method=Top1Accuracy())\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "logdir = '/tmp/.bigdl/'\n",
    "app_name = 'adam-' + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_summary = TrainSummary(log_dir=logdir, app_name=app_name)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "val_summary = ValidationSummary(log_dir=logdir, app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model = optimizer.optimize()\n",
    "print (\"Optimization Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = train_model.predict(merged_test_rdd)\n",
    "\n",
    "def map_predict_label(l):\n",
    "    if l > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def map_groundtruth_label(l):\n",
    "    return l.to_ndarray()[0]\n",
    "\n",
    "y_pred = np.array([map_predict_label(s) for s in predictions.collect()])\n",
    "\n",
    "y_true = np.array([map_groundtruth_label(s.label) for s in merged_test_rdd.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy on validation set is:  0.8756\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(0, y_pred.size):\n",
    "    if (y_pred[i] == y_true[i]):\n",
    "        correct += 1\n",
    "\n",
    "accuracy = float(correct) / y_pred.size\n",
    "print ('Prediction accuracy on validation set is: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
